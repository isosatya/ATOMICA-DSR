{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abb0a00",
   "metadata": {},
   "source": [
    "After ATOMICA was trained for 3 epoch on predicting made up affinities, here we predict more affinities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93ebf459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import models\n",
    "\n",
    "from data.dataset import PDBBindBenchmark\n",
    "from trainers.abs_trainer import Trainer\n",
    "from models import AffinityPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab8f5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration and weights\n",
    "pretrain_config_path = \"../model_weights/pretrain_model_config.json\"\n",
    "cktp_config_path = \"../checkpoints/version_14/args.json\"\n",
    "weights_path = \"../checkpoints/version_14/checkpoint/epoch35_step36.ckpt\"\n",
    "data_path = \"../data/other/divergent_items.pkl\"\n",
    "\n",
    "# Load configuration\n",
    "with open(pretrain_config_path, \"r\") as f:\n",
    "    pretrain_config = json.load(f)\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, cfg_path):\n",
    "        with open(cfg_path, \"r\") as f:\n",
    "            args = json.load(f)\n",
    "        for key, value in args.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "args = Args(cktp_config_path)\n",
    "\n",
    "add_params = {\n",
    "    \"num_affinity_pred_layers\": args.num_pred_layers,\n",
    "    \"affinity_pred_dropout\": args.pred_dropout,\n",
    "    \"affinity_pred_hidden_size\": args.pred_hidden_size,\n",
    "    \"num_projector_layers\": args.num_projector_layers,\n",
    "    \"projector_dropout\": args.projector_dropout,\n",
    "    \"projector_hidden_size\": args.projector_hidden_size,\n",
    "    \"bottom_global_message_passing\": args.bottom_global_message_passing,\n",
    "    \"global_message_passing\": args.global_message_passing,\n",
    "    \"k_neighbors\": args.k_neighbors,\n",
    "    \"dropout\": args.dropout,\n",
    "    \"block_embedding_size\": args.block_embedding_size,\n",
    "    \"block_embedding0_size\": args.block_embedding0_size,\n",
    "    \"block_embedding1_size\": args.block_embedding1_size,\n",
    "}\n",
    "\n",
    "if args.pred_nonlinearity == \"relu\":\n",
    "    add_params[\"nonlinearity\"] = torch.nn.ReLU()\n",
    "elif args.pred_nonlinearity == \"gelu\":\n",
    "    add_params[\"nonlinearity\"] = torch.nn.GELU()\n",
    "elif args.pred_nonlinearity == \"elu\":\n",
    "    add_params[\"nonlinearity\"] = torch.nn.ELU()\n",
    "else:\n",
    "    raise NotImplementedError(f\"Nonlinearity {args.pred_nonlinearity} not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "365d984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/code_projects/ATOMICA/notebooks/../models/prediction_model.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model: DenoisePretrainModel = torch.load(pretrain_ckpt, map_location=\"cpu\")\n",
      "/home/stas/micromamba/envs/atomicaenv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/stas/micromamba/envs/atomicaenv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/stas/micromamba/envs/atomicaenv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/stas/micromamba/envs/atomicaenv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/stas/micromamba/envs/atomicaenv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/stas/micromamba/envs/atomicaenv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/stas/micromamba/envs/atomicaenv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model params: hidden_size=32,\n",
      "               edge_size=32, k_neighbors=4, \n",
      "               n_layers=4, bottom_global_message_passing=False,\n",
      "               global_message_passing=True, \n",
      "               fragmentation_method=PS_300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/micromamba/envs/atomicaenv/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AffinityPredictor.load_from_pretrained(weights_path, **add_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "860d83d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffinityPredictor(\n",
      "  (mse_loss): MSELoss()\n",
      "  (block_embedding): BlockEmbedding(\n",
      "    (block_embedding): Embedding(440, 32)\n",
      "    (atom_embedding): Embedding(121, 32)\n",
      "  )\n",
      "  (edge_embedding_bottom): Embedding(4, 32)\n",
      "  (edge_embedding_top): Embedding(4, 32)\n",
      "  (encoder): ATOMICAEncoder(\n",
      "    (encoder): InteractionModule(\n",
      "      (edge_embedder): Sequential(\n",
      "        (0): GaussianEmbedding()\n",
      "        (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e | 2048 paths | 2048 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e, eps=1e-05)\n",
      "        )\n",
      "        (1): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o | 5632 paths | 5632 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=5632, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o, eps=1e-05)\n",
      "        )\n",
      "        (2): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e+16x1e+16x2o x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o+32x0o | 9216 paths | 9216 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=9216, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o, eps=1e-05)\n",
      "        )\n",
      "        (3): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o+32x0o | 11264 paths | 11264 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=11264, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (out_ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (top_encoder): ATOMICAEncoder(\n",
      "    (encoder): InteractionModule(\n",
      "      (edge_embedder): Sequential(\n",
      "        (0): GaussianEmbedding()\n",
      "        (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e | 2048 paths | 2048 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e, eps=1e-05)\n",
      "        )\n",
      "        (1): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o | 5632 paths | 5632 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=5632, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o, eps=1e-05)\n",
      "        )\n",
      "        (2): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e+16x1e+16x2o x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o+32x0o | 9216 paths | 9216 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=9216, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o, eps=1e-05)\n",
      "        )\n",
      "        (3): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o+32x0o | 11264 paths | 11264 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=11264, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (out_ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (atom_block_attn): CrossAttention(\n",
      "    (query_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (key_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (value_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "    (output_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (atom_block_attn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention_pooling): AttentionPooling(\n",
      "    (attention_layers): ModuleList(\n",
      "      (0-3): 4 x MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (norms): ModuleList(\n",
      "      (0-3): 4 x LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (graph_repr_fc): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (energy_ffn): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "    (5): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.0, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05351262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7458017 parameters in total\n",
      "7439809 trainable parameters in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum(p.numel() for p in model.parameters())} parameters in total\")\n",
    "print(f\"{sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "323476c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PDBBindBenchmark(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14bf3393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': 71.19000000000003, 'neglog_aff': 1.0}\n",
      "{'value': 60.60000000000002, 'neglog_aff': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# unpickle file \"/home/sascha/data/Projects/affinity_project/affinity_predictor/data/05_model_input/test_items.pkl\"\n",
    "with open(\"../data/other/divergent_items.pkl\", \"rb\") as f:\n",
    "    test_items = pd.read_pickle(f)\n",
    "\n",
    "print(test_items[0].get(\"affinity\"))\n",
    "print(test_items[1].get(\"affinity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93fde7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 707.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true_neglog</th>\n",
       "      <th>predicted_neglog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAAAAAAAAAAAAAAA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GGGGGGGGGGGGGGGGGGGGG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  true_neglog predicted_neglog\n",
       "0  AAAAAAAAAAAAAAAAAAAAA          1.0             None\n",
       "1  GGGGGGGGGGGGGGGGGGGGG          0.0             None"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty df with id, true and predicted affinity\n",
    "output_df = pd.DataFrame(columns=[\"id\", \"true_neglog\", \"predicted_neglog\"])\n",
    "\n",
    "# Iterate through test items and populate the DataFrame\n",
    "for item in tqdm(test_items):\n",
    "    item_id = item.get(\"id\").upper()\n",
    "    item_id = item_id.replace(\"_A_BC\", \"\")\n",
    "    true_neglog = item.get(\"affinity\").get(\"neglog_aff\")\n",
    "\n",
    "    output_df.loc[(len(output_df))] = {\n",
    "        \"id\": item_id,\n",
    "        \"true_neglog\": true_neglog,\n",
    "        \"predicted_neglog\": None,\n",
    "    }\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f0a02ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9436], device='cuda:0')\n",
      "tensor([-0.0708], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "batch_size = 1  # Adjust batch size as needed\n",
    "\n",
    "output_path = \"affinity_predictions.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    batch = PDBBindBenchmark.collate_fn([dataset[j] for j in range(i, min(i + batch_size, len(dataset)))])\n",
    "    batch = Trainer.to_device(batch, device)\n",
    "    if \"label\" not in batch:\n",
    "        with torch.no_grad():\n",
    "            dummy_pred = model.infer({**batch, \"label\": torch.zeros(1)})\n",
    "        label_shape = dummy_pred[1].shape if isinstance(dummy_pred, tuple) else dummy_pred.shape\n",
    "        batch[\"label\"] = torch.zeros(label_shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model.infer(batch)\n",
    "    # Get IDs and predictions for the whole batch\n",
    "    pdb_ids = batch.get(\"id\", [f\"sample_{i+j}\" for j in range(len(batch[\"label\"]))])\n",
    "    pred_values = prediction[1] if isinstance(prediction, tuple) else prediction\n",
    "    pred_values = pred_values.cpu().numpy().flatten()\n",
    "    # revert -log transformation if necessary\n",
    "    affinity = np.exp(-pred_values)\n",
    "    # Update the DataFrame with predictions\n",
    "    output_df.loc[i, \"predicted_neglog\"] = pred_values[0]\n",
    "\n",
    "    del batch, prediction\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b5e00dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true_neglog</th>\n",
       "      <th>predicted_neglog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAAAAAAAAAAAAAAA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GGGGGGGGGGGGGGGGGGGGG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.07078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  true_neglog predicted_neglog\n",
       "0  AAAAAAAAAAAAAAAAAAAAA          1.0         0.943614\n",
       "1  GGGGGGGGGGGGGGGGGGGGG          0.0         -0.07078"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print output df where predicted_affinity is not None\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a6ac2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.9999999999999999, p-value: nan\n"
     ]
    }
   ],
   "source": [
    "# calculate spearman correlation betwen true_affinity and predicted_affinity\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearman_corr = spearmanr(output_df[\"true_neglog\"], output_df[\"predicted_neglog\"])\n",
    "print(f\"Spearman correlation: {spearman_corr.correlation}, p-value: {spearman_corr.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32f9b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output_df to csv\n",
    "output_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efe386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atomicaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
