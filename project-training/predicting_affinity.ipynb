{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abb0a00",
   "metadata": {},
   "source": [
    "After ATOMICA was trained for 3 epoch on predicting made up affinities, here we predict more affinities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35ac0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93ebf459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /\n",
      "Added to Python path: /\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, parent_dir)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded to Python path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PDBBindBenchmark\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabs_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sys.path.append(\"..\")\n",
    "# Get the current working directory and add the parent directory to Python path\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Add the parent directory to Python path to access modules from root\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "print(f\"Added to Python path: {parent_dir}\")\n",
    "\n",
    "import models\n",
    "\n",
    "from data.dataset import PDBBindBenchmark\n",
    "from trainers.abs_trainer import Trainer\n",
    "from models import AffinityPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration and weights\n",
    "pretrain_config_path = \"./original-model-config/pretrain_model_config.json\"\n",
    "cktp_config_path = \"./model-outputs/model-outputs/version_0/args.json\"\n",
    "weights_path = \"./model-outputs/model-outputs/version_0/checkpoint/epoch33_step3060.ckpt\"\n",
    "data_path = \"./data/test_items.pkl\"\n",
    "\n",
    "# Load configuration\n",
    "with open(pretrain_config_path, \"r\") as f:\n",
    "    pretrain_config = json.load(f)\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, cfg_path):\n",
    "        with open(cfg_path, \"r\") as f:\n",
    "            args = json.load(f)\n",
    "        for key, value in args.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "args = Args(cktp_config_path)\n",
    "\n",
    "add_params = {\n",
    "    \"num_affinity_pred_layers\": args.num_pred_layers,\n",
    "    \"affinity_pred_dropout\": args.pred_dropout,\n",
    "    \"affinity_pred_hidden_size\": args.pred_hidden_size,\n",
    "    \"num_projector_layers\": args.num_projector_layers,\n",
    "    \"projector_dropout\": args.projector_dropout,\n",
    "    \"projector_hidden_size\": args.projector_hidden_size,\n",
    "    \"bottom_global_message_passing\": args.bottom_global_message_passing,\n",
    "    \"global_message_passing\": args.global_message_passing,\n",
    "    \"k_neighbors\": args.k_neighbors,\n",
    "    \"dropout\": args.dropout,\n",
    "    \"block_embedding_size\": args.block_embedding_size,\n",
    "    \"block_embedding0_size\": args.block_embedding0_size,\n",
    "    \"block_embedding1_size\": args.block_embedding1_size,\n",
    "}\n",
    "\n",
    "if args.pred_nonlinearity == \"relu\":\n",
    "    add_params[\"nonlinearity\"] = torch.nn.ReLU()\n",
    "elif args.pred_nonlinearity == \"gelu\":\n",
    "    add_params[\"nonlinearity\"] = torch.nn.GELU()\n",
    "elif args.pred_nonlinearity == \"elu\":\n",
    "    add_params[\"nonlinearity\"] = torch.nn.ELU()\n",
    "else:\n",
    "    raise NotImplementedError(f\"Nonlinearity {args.pred_nonlinearity} not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d984d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL models.affinity_predictor.AffinityPredictor was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.affinity_predictor.AffinityPredictor])` or the `torch.serialization.safe_globals([models.affinity_predictor.AffinityPredictor])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAffinityPredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madd_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DSR/ATOMICA-DSR/project-training/../models/prediction_model.py:78\u001b[0m, in \u001b[0;36mPredictionModel.load_from_pretrained\u001b[0;34m(cls, pretrain_ckpt, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrain_ckpt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 78\u001b[0m     pretrained_model: DenoisePretrainModel \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrain_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_from_pretrained(pretrained_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/atomicaenv/lib/python3.9/site-packages/torch/serialization.py:1524\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1516\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1517\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1518\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1522\u001b[0m                 )\n\u001b[1;32m   1523\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1524\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1526\u001b[0m             opened_zipfile,\n\u001b[1;32m   1527\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1531\u001b[0m         )\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL models.affinity_predictor.AffinityPredictor was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.affinity_predictor.AffinityPredictor])` or the `torch.serialization.safe_globals([models.affinity_predictor.AffinityPredictor])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "model = AffinityPredictor.load_from_pretrained(weights_path, **add_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d83d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffinityPredictor(\n",
      "  (mse_loss): MSELoss()\n",
      "  (block_embedding): BlockEmbedding(\n",
      "    (block_embedding): Embedding(440, 32)\n",
      "    (atom_embedding): Embedding(121, 32)\n",
      "  )\n",
      "  (edge_embedding_bottom): Embedding(4, 32)\n",
      "  (edge_embedding_top): Embedding(4, 32)\n",
      "  (encoder): ATOMICAEncoder(\n",
      "    (encoder): InteractionModule(\n",
      "      (edge_embedder): Sequential(\n",
      "        (0): GaussianEmbedding()\n",
      "        (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e | 2048 paths | 2048 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e, eps=1e-05)\n",
      "        )\n",
      "        (1): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o | 5632 paths | 5632 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=5632, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o, eps=1e-05)\n",
      "        )\n",
      "        (2): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e+16x1e+16x2o x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o+32x0o | 9216 paths | 9216 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=9216, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o, eps=1e-05)\n",
      "        )\n",
      "        (3): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o+32x0o | 11264 paths | 11264 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=11264, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (out_ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (top_encoder): ATOMICAEncoder(\n",
      "    (encoder): InteractionModule(\n",
      "      (edge_embedder): Sequential(\n",
      "        (0): GaussianEmbedding()\n",
      "        (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e | 2048 paths | 2048 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e, eps=1e-05)\n",
      "        )\n",
      "        (1): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o | 5632 paths | 5632 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=5632, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o, eps=1e-05)\n",
      "        )\n",
      "        (2): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e+16x1e+16x2o x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o+32x0o | 9216 paths | 9216 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=9216, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o, eps=1e-05)\n",
      "        )\n",
      "        (3): TensorProductConvLayer(\n",
      "          (tp): FullyConnectedTensorProduct(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o x 1x0e+1x1o+1x2e -> 32x0e+16x1o+16x2e+16x1e+16x2o+32x0o | 11264 paths | 11264 weights)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=11264, bias=True)\n",
      "          )\n",
      "          (norm_layer): EquivariantLayerNorm(32x0e+16x1o+16x2e+16x1e+16x2o+32x0o, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (out_ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (atom_block_attn): CrossAttention(\n",
      "    (query_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (key_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (value_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "    (output_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (atom_block_attn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention_pooling): AttentionPooling(\n",
      "    (attention_layers): ModuleList(\n",
      "      (0-3): 4 x MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (norms): ModuleList(\n",
      "      (0-3): 4 x LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (graph_repr_fc): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (energy_ffn): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "    (5): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.0, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05351262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7458017 parameters in total\n",
      "7439809 trainable parameters in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum(p.numel() for p in model.parameters())} parameters in total\")\n",
    "print(f\"{sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323476c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PDBBindBenchmark(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf3393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': 71.19000000000003, 'neglog_aff': 1.0}\n",
      "{'value': 60.60000000000002, 'neglog_aff': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# unpickle file \"/home/sascha/data/Projects/affinity_project/affinity_predictor/data/05_model_input/test_items.pkl\"\n",
    "with open(\"../data/other/divergent_items.pkl\", \"rb\") as f:\n",
    "    test_items = pd.read_pickle(f)\n",
    "\n",
    "print(test_items[0].get(\"affinity\"))\n",
    "print(test_items[1].get(\"affinity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fde7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 707.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true_neglog</th>\n",
       "      <th>predicted_neglog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAAAAAAAAAAAAAAA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GGGGGGGGGGGGGGGGGGGGG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  true_neglog predicted_neglog\n",
       "0  AAAAAAAAAAAAAAAAAAAAA          1.0             None\n",
       "1  GGGGGGGGGGGGGGGGGGGGG          0.0             None"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty df with id, true and predicted affinity\n",
    "output_df = pd.DataFrame(columns=[\"id\", \"true_neglog\", \"predicted_neglog\"])\n",
    "\n",
    "# Iterate through test items and populate the DataFrame\n",
    "for item in tqdm(test_items):\n",
    "    item_id = item.get(\"id\").upper()\n",
    "    item_id = item_id.replace(\"_A_BC\", \"\")\n",
    "    true_neglog = item.get(\"affinity\").get(\"neglog_aff\")\n",
    "\n",
    "    output_df.loc[(len(output_df))] = {\n",
    "        \"id\": item_id,\n",
    "        \"true_neglog\": true_neglog,\n",
    "        \"predicted_neglog\": None,\n",
    "    }\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a02ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9436], device='cuda:0')\n",
      "tensor([-0.0708], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "batch_size = 1  # Adjust batch size as needed\n",
    "\n",
    "output_path = \"affinity_predictions.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    batch = PDBBindBenchmark.collate_fn([dataset[j] for j in range(i, min(i + batch_size, len(dataset)))])\n",
    "    batch = Trainer.to_device(batch, device)\n",
    "    if \"label\" not in batch:\n",
    "        with torch.no_grad():\n",
    "            dummy_pred = model.infer({**batch, \"label\": torch.zeros(1)})\n",
    "        label_shape = dummy_pred[1].shape if isinstance(dummy_pred, tuple) else dummy_pred.shape\n",
    "        batch[\"label\"] = torch.zeros(label_shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model.infer(batch)\n",
    "    # Get IDs and predictions for the whole batch\n",
    "    pdb_ids = batch.get(\"id\", [f\"sample_{i+j}\" for j in range(len(batch[\"label\"]))])\n",
    "    pred_values = prediction[1] if isinstance(prediction, tuple) else prediction\n",
    "    pred_values = pred_values.cpu().numpy().flatten()\n",
    "    # revert -log transformation if necessary\n",
    "    affinity = np.exp(-pred_values)\n",
    "    # Update the DataFrame with predictions\n",
    "    output_df.loc[i, \"predicted_neglog\"] = pred_values[0]\n",
    "\n",
    "    del batch, prediction\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e00dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true_neglog</th>\n",
       "      <th>predicted_neglog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAAAAAAAAAAAAAAA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GGGGGGGGGGGGGGGGGGGGG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.07078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  true_neglog predicted_neglog\n",
       "0  AAAAAAAAAAAAAAAAAAAAA          1.0         0.943614\n",
       "1  GGGGGGGGGGGGGGGGGGGGG          0.0         -0.07078"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print output df where predicted_affinity is not None\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your DataFrame is called output_df\n",
    "mse = np.mean((output_df['true_neglog'] - output_df['predicted_neglog']) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71041038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.9999999999999999, p-value: nan\n"
     ]
    }
   ],
   "source": [
    "# calculate spearman correlation betwen true_affinity and predicted_affinity\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearman_corr = spearmanr(output_df[\"true_neglog\"], output_df[\"predicted_neglog\"])\n",
    "print(f\"Spearman correlation: {spearman_corr.correlation}, p-value: {spearman_corr.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output_df to csv\n",
    "output_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efe386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
